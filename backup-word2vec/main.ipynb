{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"TWl-NhHohfft","colab_type":"code","outputId":"e74535b8-658d-46c8-8c4a-c3d25a75256f","executionInfo":{"status":"ok","timestamp":1555152463721,"user_tz":-180,"elapsed":1064,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"bJ9ujw1CverK","colab_type":"code","outputId":"c886e8d4-2d22-4fdb-8777-e44f2c43d9eb","executionInfo":{"status":"ok","timestamp":1555152464047,"user_tz":-180,"elapsed":786,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["cd /content/drive/My Drive/bug-triage/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/bug-triage\n"],"name":"stdout"}]},{"metadata":{"id":"spvGqCEFv70Z","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"am5N6OY2u8yw","colab_type":"code","outputId":"a585113b-2731-452c-f0c8-260f719adea8","executionInfo":{"status":"ok","timestamp":1555152468020,"user_tz":-180,"elapsed":3004,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["# Stanford NLTK\n","# Gensim for word2vec\n","# Keras with Tensorflow backend\n","# Scikit-learn from Python\n","# The required packages can be imported into python as follows:\n","\n","import numpy as np\n","np.random.seed(1337)\n","import json, re, nltk, string\n","from nltk.corpus import wordnet\n","from gensim.models import Word2Vec\n","from gensim.test.utils import common_texts, get_tmpfile\n","from keras.preprocessing import sequence\n","from keras.models import Model\n","from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, BatchNormalization, TimeDistributed, InputSpec\n","from keras.layers.wrappers import Wrapper\n","from keras.optimizers import RMSprop, Adam\n","from keras.callbacks import EarlyStopping\n","from keras.utils import np_utils\n","from keras import backend as K\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import svm\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.metrics.pairwise import cosine_similarity\n","import nltk\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"e-fvqNk8hgyO","colab_type":"code","colab":{}},"cell_type":"code","source":["# The Soft Attention layer is implemented as follows: \n","# Adapted from code written by braingineer\n","\n","def make_safe(x):\n","    return K.clip(x, K.common._EPSILON, 1.0 - K.common._EPSILON)\n","\n","class ProbabilityTensor(Wrapper):\n","    \"\"\" function for turning 3d tensor to 2d probability matrix, which is the set of a_i's \"\"\"\n","    def __init__(self, dense_function=None, *args, **kwargs):\n","        self.supports_masking = True\n","        self.input_spec = [InputSpec(ndim=3)]\n","        #layer = TimeDistributed(dense_function) or TimeDistributed(Dense(1, name='ptensor_func'))\n","        layer = TimeDistributed(Dense(1, name='ptensor_func'))\n","        super(ProbabilityTensor, self).__init__(layer, *args, **kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","        self.input_spec = [InputSpec(shape=input_shape)]\n","        if K._BACKEND == 'tensorflow':\n","            if not input_shape[1]:\n","                raise Exception('When using TensorFlow, you should define '\n","                                'explicitly the number of timesteps of '\n","                                'your sequences.\\n'\n","                                'If your first layer is an Embedding, '\n","                                'make sure to pass it an \"input_length\" '\n","                                'argument. Otherwise, make sure '\n","                                'the first layer has '\n","                                'an \"input_shape\" or \"batch_input_shape\" '\n","                                'argument, including the time axis.')\n","\n","        if not self.layer.built:\n","            self.layer.build(input_shape)\n","            self.layer.built = True\n","        super(ProbabilityTensor, self).build()\n","\n","    def get_output_shape_for(self, input_shape):\n","        # b,n,f -> b,n \n","        #       s.t. \\sum_n n = 1\n","        if isinstance(input_shape, (list,tuple)) and not isinstance(input_shape[0], int):\n","            input_shape = input_shape[0]\n","\n","        return (input_shape[0], input_shape[1])\n","\n","    def squash_mask(self, mask):\n","        if K.ndim(mask) == 2:\n","            return mask\n","        elif K.ndim(mask) == 3:\n","            return K.any(mask, axis=-1)\n","\n","    def compute_mask(self, x, mask=None):\n","        if mask is None:\n","            return None\n","        return self.squash_mask(mask)\n","\n","    def call(self, x, mask=None):\n","        energy = K.squeeze(self.layer(x), 2)\n","        p_matrix = K.softmax(energy)\n","        if mask is not None:\n","            mask = self.squash_mask(mask)\n","            p_matrix = make_safe(p_matrix * mask)\n","            p_matrix = (p_matrix / K.sum(p_matrix, axis=-1, keepdims=True))*mask\n","        return p_matrix\n","\n","    def get_config(self):\n","        config = {}\n","        base_config = super(ProbabilityTensor, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","class SoftAttentionConcat(ProbabilityTensor):\n","    '''This will create the context vector and then concatenate it with the last output of the LSTM'''\n","    def get_output_shape_for(self, input_shape):\n","        # b,n,f -> b,f where f is weighted features summed across n\n","        return (input_shape[0], 2*input_shape[2])\n","\n","    def compute_mask(self, x, mask=None):\n","        if mask is None or mask.ndim==2:\n","            return None\n","        else:\n","            raise Exception(\"Unexpected situation\")\n","\n","    def call(self, x, mask=None):\n","        # b,n,f -> b,f via b,n broadcasted\n","        p_vectors = K.expand_dims(super(SoftAttentionConcat, self).call(x, mask), 2)\n","        expanded_p = K.repeat_elements(p_vectors, K.int_shape(x)[2], axis=2)\n","        context = K.sum(expanded_p * x, axis=1)\n","        last_out = x[:, -1, :]\n","        return K.concatenate([context, last_out])\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u31bTyWFuM-3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2duvAJXnhxuS","colab_type":"code","colab":{}},"cell_type":"code","source":["# The JSON file location containing the data for deep learning model training and classifier training and testing are provided as follows:\n","\n","open_bugs_json = './data/chrome/deep_data.json'\n","closed_bugs_json = './data/chrome/classifier_data_0.json'\n","# The hyperparameters required for the entire code can be initialized upfront as follows:\n","\n","#1. Word2vec parameters\n","min_word_frequency_word2vec = 5\n","embed_size_word2vec = 200\n","context_window_word2vec = 5\n","\n","#2. Classifier hyperparameters\n","numCV = 10\n","max_sentence_len = 50\n","min_sentence_length = 15\n","rankK = 10\n","batch_size = 32\n","# The bugs are loaded from the JSON file and the preprocessing is performed as follows:\n","\n","with open(open_bugs_json) as data_file:\n","    data = json.load(data_file, strict=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lqoMt3vPhx4M","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"WtxLkeIRyzE8","colab_type":"code","colab":{}},"cell_type":"code","source":["all_data = []\n","for item in data:\n","    #1. Remove \\r \n","    current_title = item['issue_title'].replace('\\r', ' ')\n","    current_desc = item['description'].replace('\\r', ' ')    \n","    #2. Remove URLs\n","    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n","    #3. Remove Stack Trace\n","    start_loc = current_desc.find(\"Stack trace:\")\n","    current_desc = current_desc[:start_loc]    \n","    #4. Remove hex code\n","    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n","    current_title= re.sub(r'(\\w+)0x\\w+', '', current_title)    \n","    #5. Change to lower case\n","    current_desc = current_desc.lower()\n","    current_title = current_title.lower()    \n","    #6. Tokenize\n","    current_desc_tokens = nltk.word_tokenize(current_desc)\n","    current_title_tokens = nltk.word_tokenize(current_title)\n","    #7. Strip trailing punctuation marks    \n","    current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]\n","    current_title_filter = [word.strip(string.punctuation) for word in current_title_tokens]      \n","    #8. Join the lists\n","    current_data = current_title_filter + current_desc_filter\n","    current_data = list(filter(None, current_data))\n","    all_data.append(current_data)  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"MeeyYGrOy4Dp","colab_type":"code","colab":{}},"cell_type":"code","source":["# A vocabulary is constructed and the word2vec model is learnt using the preprocessed data. The word2vec model provides a semantic word representation for every word in the vocabulary.\n","path = get_tmpfile(\"word2vec.model\")\n","wordvec_model = Word2Vec(all_data, min_count=min_word_frequency_word2vec, size=embed_size_word2vec, window=context_window_word2vec)\n","wordvec_model.save(\"word2vec.model\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TS9Q3rExy4Me","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"1tUpkqmCyzSH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"97a30e8f-5efa-454b-f532-f4a6d2480196","executionInfo":{"status":"ok","timestamp":1555152495156,"user_tz":-180,"elapsed":1458,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}}},"cell_type":"code","source":["wordvec_model = Word2Vec.load(\"word2vec.model\")\n","vocabulary = wordvec_model.wv.vocab\n","vocab_size = len(vocabulary)\n","print(vocab_size)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["71575\n"],"name":"stdout"}]},{"metadata":{"id":"VuRILGtyTi8s","colab_type":"text"},"cell_type":"markdown","source":["size of vocab must be 71575"]},{"metadata":{"id":"c8EgtsrvUdrJ","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","# The data used for training and testing the classifier is loaded and the preprocessing is performed as follows:\n","\n","with open(closed_bugs_json) as data_file:\n","    data = json.load(data_file, strict=False)\n","\n","all_data = []\n","all_owner = []    \n","for item in data:\n","    #1. Remove \\r \n","    current_title = item['issue_title'].replace('\\r', ' ')\n","    current_desc = item['description'].replace('\\r', ' ')\n","    #2. Remove URLs\n","    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)\n","    #3. Remove Stack Trace\n","    start_loc = current_desc.find(\"Stack trace:\")\n","    current_desc = current_desc[:start_loc]\n","    #4. Remove hex code\n","    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n","    current_title= re.sub(r'(\\w+)0x\\w+', '', current_title)\n","    #5. Change to lower case\n","    current_desc = current_desc.lower()\n","    current_title = current_title.lower()\n","    #6. Tokenize\n","    current_desc_tokens = nltk.word_tokenize(current_desc)\n","    current_title_tokens = nltk.word_tokenize(current_title)\n","    #7. Strip punctuation marks\n","    current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]\n","    current_title_filter = [word.strip(string.punctuation) for word in current_title_tokens]       \n","    #8. Join the lists\n","    current_data = current_title_filter + current_desc_filter\n","    current_data = filter(None, current_data)\n","    all_data.append(current_data)\n","    all_owner.append(item['owner'])\n","# The ten times chronological cross validation split is performed as follows:\n","\n","totalLength = len(all_data)\n","splitLength = totalLength // (numCV + 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PLEriwbKUdug","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"RaZ9heBahg1B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":643},"outputId":"aa62b95f-6ada-435b-fcf5-2f3d6e25140e","executionInfo":{"status":"error","timestamp":1555152976961,"user_tz":-180,"elapsed":13010,"user":{"displayName":"bilsen group","photoUrl":"","userId":"12431888429617065266"}}},"cell_type":"code","source":["\n","\n","\n","for i in range(1, numCV+1):\n","    train_data = all_data[:i*splitLength-1]\n","    test_data = all_data[i*splitLength:(i+1)*splitLength-1]\n","    train_owner = all_owner[:i*splitLength-1]\n","    test_owner = all_owner[i*splitLength:(i+1)*splitLength-1]\n","    # For the ith cross validation set, remove all the words that is not present in the vocabulary\n","\n","    # i = 1 # Denotes the cross validation set number\n","    updated_train_data = []    \n","    updated_train_data_length = []    \n","    updated_train_owner = []\n","    final_test_data = []\n","    final_test_owner = []\n","    for j, item in enumerate(train_data):\n","        current_train_filter = [word for word in item if word in vocabulary]\n","        if len(current_train_filter)>=min_sentence_length:  \n","            updated_train_data.append(current_train_filter)\n","            updated_train_owner.append(train_owner[j])  \n","        \n","    for j, item in enumerate(test_data):\n","        current_test_filter = [word for word in item if word in vocabulary]  \n","        if len(current_test_filter)>=min_sentence_length:\n","            final_test_data.append(current_test_filter)    \t  \n","            final_test_owner.append(test_owner[j])   \n","    # For the ith cross validation set, remove those classes from the test set, for whom the train data is not available.\n","\n","    # i = 1 # Denotes the cross validation set number\n","    # Remove data from test set that is not there in train set\n","    train_owner_unique = set(updated_train_owner)\n","    test_owner_unique = set(final_test_owner)\n","    unwanted_owner = list(test_owner_unique - train_owner_unique)\n","    updated_test_data = []\n","    updated_test_owner = []\n","    updated_test_data_length = []\n","    for j in range(len(final_test_owner)):\n","        if final_test_owner[j] not in unwanted_owner:\n","            updated_test_data.append(final_test_data[j])\n","            updated_test_owner.append(final_test_owner[j])\n","\n","    train_label = updated_train_owner\n","    unique_train_label = list(set(updated_train_owner))\n","    classes = np.array(unique_train_label)\n","    # Create the data matrix and labels required for the deep learning model training and softmax classifier as follows:\n","\n","    X_train = np.empty(shape=[len(updated_train_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n","    Y_train = np.empty(shape=[len(updated_train_owner),1], dtype='int32')\n","    # 1 - start of sentence, # 2 - end of sentence, # 0 - zero padding. Hence, word indices start with 3 \n","    for j, curr_row in enumerate(updated_train_data):\n","        sequence_cnt = 0         \n","        for item in curr_row:\n","            if item in vocabulary:\n","                X_train[j, sequence_cnt, :] = wordvec_model[item] \n","                sequence_cnt = sequence_cnt + 1                \n","                if sequence_cnt == max_sentence_len-1:\n","                    break                \n","        for k in range(sequence_cnt, max_sentence_len):\n","            X_train[j, k, :] = np.zeros((1,embed_size_word2vec))        \n","        Y_train[j,0] = unique_train_label.index(updated_train_owner[j])\n","\n","    X_test = np.empty(shape=[len(updated_test_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n","    Y_test = np.empty(shape=[len(updated_test_owner),1], dtype='int32')\n","    # 1 - start of sentence, # 2 - end of sentence, # 0 - zero padding. Hence, word indices start with 3 \n","    for j, curr_row in enumerate(updated_test_data):\n","        sequence_cnt = 0          \n","        for item in curr_row:\n","            if item in vocabulary:\n","                X_test[j, sequence_cnt, :] = wordvec_model[item] \n","                sequence_cnt = sequence_cnt + 1                \n","                if sequence_cnt == max_sentence_len-1:\n","                        break                \n","        for k in range(sequence_cnt, max_sentence_len):\n","            X_test[j, k, :] = np.zeros((1,embed_size_word2vec))        \n","        Y_test[j,0] = unique_train_label.index(updated_test_owner[j])\n","        \n","    y_train = np_utils.to_categorical(Y_train, len(unique_train_label))\n","    y_test = np_utils.to_categorical(Y_test, len(unique_train_label))\n","    # Construct the architecture for deep bidirectional RNN model using Keras library as follows:\n","\n","    input = Input(shape=(max_sentence_len,), dtype='int32')\n","    sequence_embed = Embedding(vocab_size, embed_size_word2vec, input_length=max_sentence_len)(input)\n","\n","    forwards_1 = LSTM(1024, return_sequences=True, dropout_U=0.2)(sequence_embed)\n","    attention_1 = SoftAttentionConcat()(forwards_1)\n","    after_dp_forward_5 = BatchNormalization()(attention_1)\n","\n","    backwards_1 = LSTM(1024, return_sequences=True, dropout_U=0.2, go_backwards=True)(sequence_embed)\n","    attention_2 = SoftAttentionConcat()(backwards_1)\n","    after_dp_backward_5 = BatchNormalization()(attention_2)\n","                \n","    merged = merge([after_dp_forward_5, after_dp_backward_5], mode='concat', concat_axis=-1)\n","    after_merge = Dense(1000, activation='relu')(merged)\n","    after_dp = Dropout(0.4)(after_merge)\n","    output = Dense(len(train_label), activation='softmax')(after_dp)                \n","    model = Model(input=input, output=output)\n","    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n","\n","    # Train the deep learning model and test using the classifier as follows:\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n","    hist = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=200)              \n","        \n","    predict = model.predict(X_test)        \n","    accuracy = []\n","    sortedIndices = []\n","    pred_classes = []\n","    for ll in predict:\n","        sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n","    for k in range(1, rankK+1):\n","        id = 0\n","        trueNum = 0\n","        for sortedInd in sortedIndices:            \n","            pred_classes.append(classes[sortedInd[:k]])\n","            if y_test[id] in classes[sortedInd[:k]]:\n","                trueNum += 1\n","            id += 1\n","        accuracy.append((float(trueNum) / len(predict)) * 100)\n","    print('Test accuracy:', accuracy)       \n","\n","    train_result = hist.history        \n","    print(train_result)\n","    # To compare the deep learning based features, term frequency based bag-of-words model features are constructed as follows:\n","\n","    train_data = []\n","    for item in updated_train_data:\n","        train_data.append(' '.join(item))\n","        \n","    test_data = []\n","    for item in updated_test_data:\n","        test_data.append(' '.join(item))\n","\n","    vocab_data = []\n","    for item in vocabulary:\n","        vocab_data.append(item)\n","\n","    # Extract tf based bag of words representation\n","    tfidf_transformer = TfidfTransformer(use_idf=False)\n","    count_vect = CountVectorizer(min_df=1, vocabulary= vocab_data,dtype=np.int32)\n","\n","    train_counts = count_vect.fit_transform(train_data)       \n","    train_feats = tfidf_transformer.fit_transform(train_counts)\n","    print(train_feats.shape)\n","\n","    test_counts = count_vect.transform(test_data)\n","    test_feats = tfidf_transformer.transform(test_counts)\n","    print(test_feats.shape)\n","    print(\"=======================\")\n","    # Four baseline classifiers are built over the bag-of-words features:\n","\n","    # Naive Bayes\n","    # Support Vector Machine\n","    # Cosine similarity\n","    # Softmax classifier\n","    # All the classifiers are implemented using the scikit package of python. The Naive Bayes classifier is implemented as follows:\n","    classifierModel = MultinomialNB(alpha=0.01)        \n","    classifierModel = OneVsRestClassifier(classifierModel).fit(train_feats, updated_train_owner)\n","    predict = classifierModel.predict_proba(test_feats)  \n","    classes = classifierModel.classes_  \n","\n","    accuracy = []\n","    sortedIndices = []\n","    pred_classes = []\n","    for ll in predict:\n","        sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n","    for k in range(1, rankK+1):\n","        id = 0\n","        trueNum = 0\n","        for sortedInd in sortedIndices:            \n","            if y_test[id] in classes[sortedInd[:k]]:\n","                trueNum += 1\n","                pred_classes.append(classes[sortedInd[:k]])\n","            id += 1\n","        accuracy.append((float(trueNum) / len(predict)) * 100)\n","    print(accuracy)\n","    # The implementation of Support Vector Machine is as follows:\n","\n","    classifierModel = svm.SVC(probability=True, verbose=False, decision_function_shape='ovr', random_state=42)\n","    classifierModel.fit(train_feats, updated_train_owner)\n","    predict = classifierModel.predict(test_feats)\n","    classes = classifierModel.classes_ \n","\n","    accuracy = []\n","    sortedIndices = []\n","    pred_classes = []\n","    for ll in predict:\n","        sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n","    for k in range(1, rankK+1):\n","        id = 0\n","        trueNum = 0\n","        for sortedInd in sortedIndices:            \n","            if y_test[id] in classes[sortedInd[:k]]:\n","                trueNum += 1\n","                pred_classes.append(classes[sortedInd[:k]])\n","            id += 1\n","        accuracy.append((float(trueNum) / len(predict)) * 100)\n","    print(accuracy)\n","    # The implementation of cosine similarity based classification is provided as follows:\n","    trainls = updated_train_owner\n","\n","    predict = cosine_similarity(test_feats, train_feats)\n","    classes = np.array(trainls)\n","    classifierModel = []\n","\n","    accuracy = []\n","    sortedIndices = []\n","    pred_classes = []\n","    for ll in predict:\n","        sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n","    for k in range(1, rankK+1):\n","        id = 0\n","        trueNum = 0\n","        for sortedInd in sortedIndices:            \n","            if y_test[id] in classes[sortedInd[:k]]:\n","                trueNum += 1\n","                pred_classes.append(classes[sortedInd[:k]])\n","            id += 1\n","        accuracy.append((float(trueNum) / len(predict)) * 100)\n","    print(accuracy)\n","    # The softmax (regression) based classification is performed as follows:\n","\n","    classifierModel = LogisticRegression(solver='lbfgs', penalty='l2', tol=0.01)\n","    classifierModel = OneVsRestClassifier(classifierModel).fit(train_feats, updated_train_owner)\n","    predict = classifierModel.predict(test_feats)\n","    classes = classifierModel.classes_ \n","\n","    accuracy = []\n","    sortedIndices = []\n","    pred_classes = []\n","    for ll in predict:\n","        sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n","    for k in range(1, rankK+1):\n","        id = 0\n","        trueNum = 0\n","        for sortedInd in sortedIndices:            \n","            if y_test[id] in classes[sortedInd[:k]]:\n","                trueNum += 1\n","                pred_classes.append(classes[sortedInd[:k]])\n","            id += 1\n","    accuracy.append((float(trueNum) / len(predict)) * 100)\n","    print(accuracy)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(1024, return_sequences=True, recurrent_dropout=0.2)`\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-fdf582c2a1bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mforwards_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mattention_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftAttentionConcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwards_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mafter_dp_forward_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mbackwards_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer batch_normalization_1: expected ndim=3, found ndim=2"]}]},{"metadata":{"id":"B9VZk5J_hg3q","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}